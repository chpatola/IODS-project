# Dimensionality reduction techniques

## Analysis task 1
```{r}
humanDev <- read.csv("data/human.csv")
head(humanDev, 2)
library(GGally)
humanDev_numeric <- humanDev[,-1]
head(humanDev_numeric,2)
ggpairs(humanDev_numeric)

``` 

Looking at the distributions of the data and the relationships between them, we can say that many of the variables do not have connections with each other but years_ed and life exp seem to have a positive realtinship with a correlation of 0.79. Also birth_rate and mat_mort seem to have a relationship with a correlation of 0.76. Life exp and birth rate have a negative relationship with a correlation of 0.73 and mat mortality and years educ also have a negative relationship. The negative correlation is 0.74. Birt rate is also negatively correlated with years educ (0.7).

Concerning the distribution, only years education seems to have a normal distribution. fem_male educ, fem_male_job and parliament also come close to a normal distribution but the other vaiables have skewed distributions.

## Analysis task 2
```{r}  
pca_human <- prcomp(humanDev_numeric)

summary(pca_human) #Almost all variation can be explained by the first PC

biplot(pca_human, choices = 1:2, col=c("black","blue"))

```

## Analysis task 3


```{r fig.cap="hi"}  
#Standardize dataset
humanDev_numeric_stand <- scale(humanDev_numeric)
summary(humanDev_numeric_stand)

pca_human_stand <- prcomp(humanDev_numeric_stand)

summary(pca_human_stand) #The variation is explained much more evenly across the components than with the unstandardized dataset

biplot(pca_human_stand, choices = 1:2, col=c("black","blue"))


```

When we standardized the dataset, the mean became 0 and the standard devision 1. In the unstandardized datasets, variables with more variations due to broader range of values (like GNI numeric) are given much more importance in the PCA. When we standardize the variables, differences in scale do not matter anymore and we get a more balanced picture of how the different variables affet the variations in the model. We clearly see the difference between the standardized and not standardized data in our biplots and the model summaries. In the unstandardized version, GNI numeric explains most of the variation and as it affects PC1, PC1 accounts for as much as >99% of the variation. In the standardized version, we need all 8 PC:s to get up to the same explenational percent.

## Analysis task 4
My interpretation on the biplot components 1 and 2 from the standardized data is: 
PC1 and PC2 do together explain approx 70% of the variation in the model. When one decide on how many components to use in a model, there are different ways of doing it. One way is to use components with a sd of at least 0.7 (here it would mean PC1-PC4) but one can also, for example, use graphical ways of doing it. If we now stick to only the two values PC1 and PC2 and focus on the biplot graph of them, we can say that fem_male_job and parliamanet are correlated and they affect PC2. Mat_mort and birth_rate are also corelated and they affect PC1. In general, a little angle between feature arrows indicate that they are correlated and the the PC they are in parallell with indicate that they affect that PC. The length os the arrow corresponds to the feature sd. In our model there is not so big differences between the standard deviations of the features.


## Analysis task 5
```{r}
library(FactoMineR)
library(dplyr)
library(tidyr)
data(tea)

str(tea)
dim(tea)

chosen_tea <- tea[,c(3,7,14,15)]
str(chosen_tea)

mca_model <- MCA(chosen_tea)
summary(mca_model)
plot(mca_model, invisible=c("ind"))

```

The tea dataset contains 300 rows and 36 columns. The individuals not home is different from the rest of the individulas. Probably because if you are not at home (ie for example at work), you are less likely to sit down with a good cup of tea. The rst of the individuals seem to have a relatiosnhip. Interesting is that out of them, lemon and no sugar are further from each other. If you have lemon in your tea, you might be more likely to alo have sugar in it.

